<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - p(doom) and Artificial General Intelligence (AGI)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <style>

    </style>
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
    
      <div class="row">
        <article class="col-9">
         <h1>p(doom) and Artificial General Intelligence (AGI)</h1>
<p>What is the probability of AI taking over or even the complete destruction of 
civilization? This question has become a popular topic with the growth of generative
AI in the past couple of years<sup id="fnref:FAST"><a class="footnote-ref" href="#fn:FAST">1</a></sup>. Known as p(doom), or the probability of an AI
apocalypse, is on a scale from 1 to 100 that AI will develop into a malignant 
superintelligence. While p(doom) is a sensationalist metric, the disturbing reality 
that many of the top AI scientists, engineers, and executives all have p(doom) 
estimates that are surprising high<sup id="fnref:PAUSEAI"><a class="footnote-ref" href="#fn:PAUSEAI">2</a></sup> with the mean estimate from AI engineers
of a 40% p(doom) score<sup id="fnref:ENGINEERS_SURVEY"><a class="footnote-ref" href="#fn:ENGINEERS_SURVEY">3</a></sup>. </p>
<h2>Exercise</h2>
<p>Please go to this <a href="https://docs.google.com/forms/d/e/1FAIpQLSd3-b7wS3ZOqDtpHVbO3p8PqfsZQKuuZEjgJXp6wCRzpCdHlA/viewform?usp=sf_link">survey</a> to submit your p(doom) number.</p>
<h2>Artificial General Intelligence (AGI)</h2>
<p>Artificial General Intelligence (AGI)<sup id="fnref:AGI"><a class="footnote-ref" href="#fn:AGI">4</a></sup> is the concept of an AI that surpasses human intelligence 
across a wide range cognitive tasks. Related to this idea of AGI is the fear that once AI reaches
this threshold, AI will bootstrap itself to greater intelligence in a rapid fashion and become an 
intelligence that is beyond anything we can conceptualize. </p>
<h2>AI Alignment</h2>
<p>In a 2003 paper<sup id="fnref:BOSTROM"><a class="footnote-ref" href="#fn:BOSTROM">5</a></sup>, Nick Bostrom provided an example of a superintelligent AI that has an 
initial motivation to maximize the manufacturing of paperclips could eventually ignore any human
goals and turn the entire planet into manufacturing paperclips to the detriment of all life on Earth.</p>
<p>Minimizing p(doom) through aligning AGI's goals with our own goals and values is an active area
of research in both academic and by AI companies.</p>
<h2>Footnotes</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:FAST">
<p><a href="https://www.fastcompany.com/90994526/pdoom-explained-how-to-calculate-your-score-on-ai-apocalypse-metric">P(doom) is AI’s latest apocalypse metric. Here’s how to calculate your score</a>&#160;<a class="footnote-backref" href="#fnref:FAST" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:PAUSEAI">
<p><a href="https://pauseai.info/pdoom">List of p(doom) values</a>&#160;<a class="footnote-backref" href="#fnref:PAUSEAI" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:ENGINEERS_SURVEY">
<p><a href="https://elemental-croissant-32a.notion.site/State-of-AI-Engineering-2023-20c09dc1767f45988ee1f479b4a84135#694f89e86f9148cb855220ec05e9c631">State of AI Engineering 2023</a>&#160;<a class="footnote-backref" href="#fnref:ENGINEERS_SURVEY" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:AGI">
<p><a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">Artificial General Intelligence</a>&#160;<a class="footnote-backref" href="#fnref:AGI" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:BOSTROM">
<p><a href="https://nickbostrom.com/ethics/ai">Ethical Issues in Advanced Artificial Intelligence</a>&#160;<a class="footnote-backref" href="#fnref:BOSTROM" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:BENEFIT_RISK">
<p><a href="https://futureoflife.org/ai/benefits-risks-of-artificial-intelligence/">Benefits &amp; Risks of Artificial Intelligence</a>&#160;<a class="footnote-backref" href="#fnref:BENEFIT_RISK" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
</ol>
</div>
        </article>
        <div class="col-3">
          <h4>Navigation</h4>
          <ol><li><a href="/wolfcon-2024-ai-workshop/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/wolfcon-2024-ai-workshop/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a><ul><li><a href="bias.html">Bias and AI</a></li><li><a href="academic-fraud.html">Academic Fraud - Plagiarism and Acceptable Use</a></li><li><a href="creator-attribution.html">Creator Attribution and Copyright</a></li><li><a href="privacy.html">Privacy Concerns in Large Language Models</a></li><li><a href="guidelines.html">Guidelines for Incorporating AI</a></li><li><a href="carbon-footprint.html">Carbon Footprint of LLMs</a></li><li><a href="deepfakes.html">Deepfakes</a></li><li><a href="ai-slop.html">AI Slop</a></li><li><a href="hallucinations-llms.html">Hallucinations and Generative AI</a></li><li><strong>p(doom) and Artificial General Intelligence (AGI)</strong></li></ul></li><li><a href="/wolfcon-2024-ai-workshop/exploring-llms/index.html">Exploring Large Language Models</a></li><li><a href="/wolfcon-2024-ai-workshop/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/wolfcon-2024-ai-workshop/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/wolfcon-2024-ai-workshop/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
        </div>
      </div>

    </div>
    <footer style="background-color: #0a3a85; color: white;" >
      <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/jermnelson/wolfcon-02024-ai">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
  </body>
</html>