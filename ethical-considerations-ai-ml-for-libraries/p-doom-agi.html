<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - p(doom) and Artificial General Intelligence (AGI)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <style>

    </style>
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
    
      <div class="row">
        <article class="col-9">
         <h1>p(doom) and Artificial General Intelligence (AGI)</h1>
<p>What is the probability of AI taking over or even causing the complete destruction of 
civilization? This question has become a popular topic with the rapid development of generative
AI in recent years<sup id="fnref:FAST"><a class="footnote-ref" href="#fn:FAST">1</a></sup>. Known as p(doom), or the probability of an AI
apocalypse, this metric is measured on a scale from 1 to 100, with a higher number representing 
the likelihood that AI could evolve into a malignant superintelligence. </p>
<p>While p(doom) is a sensationalist metric, the disturbing reality is
that many of the top AI scientists, engineers, and executives all have surprising 
high<sup id="fnref:PAUSEAI"><a class="footnote-ref" href="#fn:PAUSEAI">2</a></sup> p(doom) estimates with the mean estimate from AI engineers
of a 40% p(doom) score<sup id="fnref:ENGINEERS_SURVEY"><a class="footnote-ref" href="#fn:ENGINEERS_SURVEY">3</a></sup>. </p>
<h2>Exercise</h2>
<p>Please go to this <a href="https://docs.google.com/forms/d/e/1FAIpQLSd3-b7wS3ZOqDtpHVbO3p8PqfsZQKuuZEjgJXp6wCRzpCdHlA/viewform?usp=sf_link">survey</a> to submit your p(doom) number.</p>
<h2>Artificial General Intelligence (AGI)</h2>
<p>Artificial General Intelligence (AGI)<sup id="fnref:AGI"><a class="footnote-ref" href="#fn:AGI">4</a></sup> refers to an AI that surpasses human intelligence 
across a wide range cognitive tasks. A significant concern associated with AGI is that once AI reaches
this threshold, it could rapidly improve itself, becoming an intelligence far beyond anything 
humans can currently comprehend.</p>
<h2>AI Alignment</h2>
<p>Minimizing p(doom) through aligning AGI's goals with our own goals and values is an active area
of research in both academic and by AI companies and should be included in any broad analysis of
the benefits of AGI<sup id="fnref:BENEFIT_RISK"><a class="footnote-ref" href="#fn:BENEFIT_RISK">6</a></sup>.</p>
<p>To illustrate misalignment of goals between an AGI and humans, in a 2003 paper<sup id="fnref:BOSTROM"><a class="footnote-ref" href="#fn:BOSTROM">5</a></sup>, Nick Bostrom 
provided an example of a superintelligent AI that has an initial goal to maximize the paperclips 
manufacturing. Such an AI could potentially ignore any human values, and in a worst-case scenario,
converting the entire planet into paperclip manufacturing, to the detriment of all life on Earth.</p>
<h2>Footnotes</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:FAST">
<p><a href="https://www.fastcompany.com/90994526/pdoom-explained-how-to-calculate-your-score-on-ai-apocalypse-metric">P(doom) is AI’s latest apocalypse metric. Here’s how to calculate your score</a>&#160;<a class="footnote-backref" href="#fnref:FAST" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:PAUSEAI">
<p><a href="https://pauseai.info/pdoom">List of p(doom) values</a>&#160;<a class="footnote-backref" href="#fnref:PAUSEAI" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:ENGINEERS_SURVEY">
<p><a href="https://elemental-croissant-32a.notion.site/State-of-AI-Engineering-2023-20c09dc1767f45988ee1f479b4a84135#694f89e86f9148cb855220ec05e9c631">State of AI Engineering 2023</a>&#160;<a class="footnote-backref" href="#fnref:ENGINEERS_SURVEY" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:AGI">
<p><a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">Artificial General Intelligence</a>&#160;<a class="footnote-backref" href="#fnref:AGI" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:BOSTROM">
<p><a href="https://nickbostrom.com/ethics/ai">Ethical Issues in Advanced Artificial Intelligence</a>&#160;<a class="footnote-backref" href="#fnref:BOSTROM" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:BENEFIT_RISK">
<p><a href="https://futureoflife.org/ai/benefits-risks-of-artificial-intelligence/">Benefits &amp; Risks of Artificial Intelligence</a>&#160;<a class="footnote-backref" href="#fnref:BENEFIT_RISK" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
</ol>
</div>
        </article>
        <div class="col-3">
          <h4>Navigation</h4>
          <ol><li><a href="/wolfcon-2024-ai-workshop/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/wolfcon-2024-ai-workshop/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a><ul><li><a href="bias.html">Bias and AI</a></li><li><a href="academic-fraud.html">Academic Fraud - Plagiarism and Acceptable Use</a></li><li><a href="creator-attribution.html">Creator Attribution and Copyright</a></li><li><a href="privacy.html">Privacy Concerns in Large Language Models</a></li><li><a href="guidelines.html">Guidelines for Incorporating AI</a></li><li><a href="carbon-footprint.html">Carbon Footprint of LLMs</a></li><li><a href="deepfakes.html">Deepfakes</a></li><li><a href="ai-slop.html">AI Slop</a></li><li><a href="hallucinations-llms.html">Hallucinations and Generative AI</a></li><li><strong>p(doom) and Artificial General Intelligence (AGI)</strong></li></ul></li><li><a href="/wolfcon-2024-ai-workshop/exploring-llms/index.html">Exploring Large Language Models</a></li><li><a href="/wolfcon-2024-ai-workshop/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/wolfcon-2024-ai-workshop/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/wolfcon-2024-ai-workshop/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
        </div>
      </div>

    </div>
    <footer style="background-color: #0a3a85; color: white;" >
      <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/jermnelson/wolfcon-02024-ai">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
  </body>
</html>