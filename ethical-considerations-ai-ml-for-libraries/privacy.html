<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - Privacy</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <style>

    </style>
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
      <div class="row">
        <article class="col-9">
         <h1>Privacy</h1>
<p>With the wide-spread release of Large Language Models (LLMs) by various corporate and 
other organizations, there have emerged a number of significant privacy issues<sup id="fnref:PRIVACY_LLM"><a class="footnote-ref" href="#fn:PRIVACY_LLM">1</a></sup>, including:</p>
<ul>
<li>Personal details in Training data</li>
<li>Personal details in logged prompts</li>
<li>Re-identification of individuals</li>
</ul>
<h2>Training Data</h2>
<p>The privacy of individuals may be compromised if their personal 
information is part of the massive amount of textual and other data used to train 
these models. Inclusion of personal information in the training data is problematic 
because the individuals in question have not consented to have their data included and
even if the corporations have included counter-measures to keep personal data private,
there exist prompt engineering attacks that can reveal the raw training data from such 
models as ChatGPT.<sup id="fnref:SCALEABLE"><a class="footnote-ref" href="#fn:SCALEABLE">2</a></sup>   </p>
<h2>Inference Data privacy i.e. Revealing Too Much in your prompts</h2>
<p>As people use LLMs for such personal issues and topics like relationship advice, medical
information on current or potential conditions, and psychological counseling, the privacy 
risks of disclosing this information through prompts continues to grow.<sup id="fnref:GEN_AI_PRIVACY"><a class="footnote-ref" href="#fn:GEN_AI_PRIVACY">3</a></sup>.</p>
<p>These prompts are often collected and logged by organizations like OpenAI or Google and the
inclusion of sensitive details that are potentially damaging to the reputation and mental
health of the users that</p>
<h2>Re-identification</h2>
<p>Even if the training data has been anonymized for personal details, by combining and collecting
responses from LLMs, individuals could potential be identified thereby comprising their privacy
when using the LLM. </p>
<div class="footnote">
<hr />
<ol>
<li id="fn:PRIVACY_LLM">
<p><a href="https://arxiv.org/abs/2310.10383">Privacy in Large Language Models: Attacks, Defenses and Future Directions</a>&#160;<a class="footnote-backref" href="#fnref:PRIVACY_LLM" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:SCALEABLE">
<p><a href="https://arxiv.org/abs/2311.17035">Scalable Extraction of Training Data from (Production) Language Models</a>&#160;<a class="footnote-backref" href="#fnref:SCALEABLE" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:GEN_AI_PRIVACY">
<p><a href="https://www.axios.com/2024/03/14/generative-ai-privacy-problem-chatgpt-openai">Generative AI's privacy problem</a>&#160;<a class="footnote-backref" href="#fnref:GEN_AI_PRIVACY" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div>
        </article>
        <div class="col-3">
          <h4>Navigation</h4>
          <ol><li><a href="/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a></li><li><a href="/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a><ul><li><a href="bias.html">Bias and AI</a></li><li><a href="academic-fraud.html">Academic Fraud - Plagiarism and Acceptable Use</a></li><li><a href="creator-attribution.html">Creator Attribution and Copyright</a></li><li><strong>Privacy</strong></li><li><a href="guidelines.html">Guidelines for Incorporating AI</a></li><li><a href="carbon-footprint.html">Carbon Footprint of LLMs</a></li><li><a href="deepfakes.html">Deepfakes</a></li><li><a href="ai-slop.html">AI Slop</a></li><li><a href="hallucinations-llms.html">Hallucinations and Generative AI</a></li><li><a href="p-doom-agi.html">p(doom) and Artificial General Intelligence (AGI)</a></li></ul></li><li><a href="/exploring-llms/index.html">Exploring Large Language Models</a></li><li><a href="/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
        </div>
      </div>
    </div>
    <footer style="background-color: #0a3a85; color: white;" >
     <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/jermnelson/wolfcon-02024-ai">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
  </body>
</html>