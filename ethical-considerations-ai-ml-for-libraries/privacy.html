<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - Privacy Concerns in Large Language Models</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    
   
    <style>

    </style>
   
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
    
<div class="row">
  <article class="col-9">
    <h1>Privacy Concerns in Large Language Models</h1>
<p>With the widespread release of Large Language Models (LLMs) by various<br />
organizations, significant privacy issues<sup id="fnref:PRIVACY_LLM"><a class="footnote-ref" href="#fn:PRIVACY_LLM">1</a></sup> have emerged, including:</p>
<ul>
<li><strong>Personal details in training data</strong>: Names, addresses, and financial information may
  be included in the training data for these models. </li>
<li><strong>Logged prompts containing sensitive information</strong>: User inputs, including private 
  details, can be logged by the companies managing these LLMs. </li>
<li><strong>Re-identification of individuals:</strong> Even in anonymized training data, individuals can potentially be
  re-identified through the model outputs and usage patterns.</li>
</ul>
<h2>Training Data</h2>
<p>The privacy of individuals can be compromised if their personal 
information is included in the large datasets used to train LLMs. Since people have often not 
consented to this data usage, and even anonymization techniques are vulnerable to prompt 
engineering attacks, there is a significant risk that raw training data, containing sensitive 
details, can be exposed by models as OpenAI's ChatGPT<sup id="fnref:SCALEABLE"><a class="footnote-ref" href="#fn:SCALEABLE">2</a></sup>.  </p>
<h2>Inference Data Privacy: Revealing Too Much in Prompts</h2>
<p>As people engage with LLMs on personal topics such relationship counseling, medical advice, 
and psychological concerns, the privacy risks associated with disclosing sensitive information
increase.<sup id="fnref:GEN_AI_PRIVACY"><a class="footnote-ref" href="#fn:GEN_AI_PRIVACY">3</a></sup>.</p>
<p>These prompts are often collected and logged by organizations like OpenAI or Google, creating 
privacy concerns if the inclusion of sensitive details becomes part of the future training data for 
these models.</p>
<h2>Re-identification</h2>
<p>Even when personal information in LLMs is anonymized, it can be vulnerable to re-identification. By 
analyzing LLMs responses, individuals could potential be identified thereby comprising their privacy
when interacting with theses AI systems. </p>
<div class="footnote">
<hr />
<ol>
<li id="fn:PRIVACY_LLM">
<p><a href="https://arxiv.org/abs/2310.10383">Privacy in Large Language Models: Attacks, Defenses and Future Directions</a>&#160;<a class="footnote-backref" href="#fnref:PRIVACY_LLM" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:SCALEABLE">
<p><a href="https://arxiv.org/abs/2311.17035">Scalable Extraction of Training Data from (Production) Language Models</a>&#160;<a class="footnote-backref" href="#fnref:SCALEABLE" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:GEN_AI_PRIVACY">
<p><a href="https://www.axios.com/2024/03/14/generative-ai-privacy-problem-chatgpt-openai">Generative AI's privacy problem</a>&#160;<a class="footnote-backref" href="#fnref:GEN_AI_PRIVACY" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div>
  </article>
  <div class="col-3">
    <h4>Navigation</h4>
     <ol><li><a href="/wolfcon-2024-ai-workshop/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/wolfcon-2024-ai-workshop/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a><ul><li><a href="bias.html">Bias and AI</a></li><li><a href="academic-fraud.html">Academic Fraud - Plagiarism and Acceptable Use</a></li><li><a href="creator-attribution.html">Creator Attribution and Copyright</a></li><li><strong>Privacy Concerns in Large Language Models</strong></li><li><a href="guidelines.html">Guidelines for Incorporating AI</a></li><li><a href="carbon-footprint.html">Carbon Footprint of LLMs</a></li><li><a href="deepfakes.html">Deepfakes</a></li><li><a href="ai-slop.html">AI Slop</a></li><li><a href="hallucinations-llms.html">Hallucinations and Generative AI</a></li><li><a href="p-doom-agi.html">p(doom) and Artificial General Intelligence (AGI)</a></li></ul></li><li><a href="/wolfcon-2024-ai-workshop/exploring-llms/index.html">Exploring Large Language Models</a></li><li><a href="/wolfcon-2024-ai-workshop/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/wolfcon-2024-ai-workshop/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/wolfcon-2024-ai-workshop/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
  </div>
</div>

    </div>
    <footer style="background-color: #0a3a85; color: white;" >
      <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/jermnelson/wolfcon-02024-ai">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    
  </body>
</html>