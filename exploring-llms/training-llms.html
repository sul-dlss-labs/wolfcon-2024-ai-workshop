<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - Training Large Language Models (LLMs)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    
   
    <style>

    </style>
   
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
    
<div class="row">
  <article class="col-9">
    <h1>Training Large Language Models (LLMs)</h1>
<p>One methods for customizing and providing better context when using LLMs is to train the
LLM on your data. <a href="https://openai.com/">OpenAI</a> allows you to train one of their models through their API
and custom GPTs. </p>
<p>Running a local version <a href="https://ai.meta.com/">Llama</a> allows to directly train the 
model (depending on the capacities of your local computer).</p>
<h2>OpenAI Training</h2>
<p>To train or fine-tune a ChatGPT model, the OpenAI API provides the following endpoints:</p>
<ul>
<li>Create an Embedding</li>
<li>Select a model</li>
</ul>
<p><strong>NOTE:</strong> Fine-tuning a ChatGPT model is significantly more cost than just using OpenAI's
API model inference. </p>
<h2>Fine-tuning LLMs with Llama.cpp</h2>
<p>The <a href="https://github.com/ggerganov/llama.cpp">LLaMA.cpp</a> project allows you to run and fine-tune
LLaMA models on your local computer.  <a href="https://github.com/ggerganov/llama.cpp">LLaMA.cpp</a> provides more 
lower-level access to these Open source LLMs. There is
also a <a href="https://github.com/abetlen/llama-cpp-python">Python SDK</a> for integrating with 
<a href="https://github.com/folio-labs/edge-ai">edge-ai</a>. <a href="https://github.com/ggerganov/llama.cpp">LLaMA.cpp</a> provides an
OpenAI API compatible server that also allows us to integrate with <a href="dspy-docs.vercel.app/">DSPy</a></p>
<p><a href="https://github.com/ggerganov/llama.cpp">LLaMA.cpp</a> can also be run with [Docker]<sup id="fnref:DOCKER"><a class="footnote-ref" href="#fn:DOCKER">1</a></sup> on your computer if you 
don't want or can't compile the C++ source code to run on your computer.</p>
<h3>Downloading a LLaMA-based Model</h3>
<p><a href="https://github.com/ggerganov/llama.cpp">LLaMA.cpp</a> uses the <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">GGUF</a> 
format for model inference and training. Look for GGUF models on <a href="https://huggingface.co/">HuggingFace</a>
and if you compiled <a href="https://github.com/ggerganov/llama.cpp">LLaMA.cpp</a> with <code>libcurl</code> support, you can use the <code>llama-cli</code> command-line
client to download:</p>
<p><code>./llama-cli --hf-repo lmstudio-community/Reflection-Llama-3.1-70B-GGUF --hf-file Reflection-Llama-3.1-70B-GGUF.gguf</code></p>
<p>If <code>libcurl</code> hasn't been installed, you can usually directly download the models directly from <a href="https://huggingface.co/">HuggingFace</a> and
store in the <code>/models</code> directory under the main <a href="https://github.com/ggerganov/llama.cpp">LLaMA.cpp</a>.</p>
<h3>Running the Model in Inference Mode</h3>
<div class="footnote">
<hr />
<ol>
<li id="fn:DOCKER">
<p><a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/docker.md">LLaMA.cpp with Docker</a>&#160;<a class="footnote-backref" href="#fnref:DOCKER" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
  </article>
  <div class="col-3">
    <h4>Navigation</h4>
     <ol><li><a href="/wolfcon-2024-ai-workshop/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/wolfcon-2024-ai-workshop/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/exploring-llms/index.html">Exploring Large Language Models</a><ul><li><a href="chatgpt.html">OpenAI's ChatGPT</a></li><li><a href="claude.html">Anthropic's Claude</a></li><li><a href="gemini.html">Google Gemini</a></li><li><a href="local-llama.html">Meta's Llama</a></li><li><a href="generative-video.html">Generative AI Images and Videos</a></li><li><a href="prompt-engineering.html">Prompt Engineering</a></li><li><a href="retrieval-augmented-generation.html">Retrieval Augmented Generation (RAG)</a></li><li><strong>Training Large Language Models (LLMs)</strong></li></ul></li><li><a href="/wolfcon-2024-ai-workshop/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/wolfcon-2024-ai-workshop/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/wolfcon-2024-ai-workshop/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
  </div>
</div>

    </div>
    <footer style="background-color: #0a3a85; color: white;" >
      <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/sul-dlss-labs/wolfcon-2024-ai-workshop/">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    
  </body>
</html>