<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - Training Large Language Models (LLMs)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
    
   
    <style>

    </style>
   
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
    
<div class="row">
  <article class="col-9">
    <h1>Training Large Language Models (LLMs)</h1>
<p>One method for customizing and providing better context when using LLMs is to train the
LLM on your data. There are a number methods for fine-tuning LLMs including low-rank adaptation or LoRA<sup id="fnref:LORA"><a class="footnote-ref" href="#fn:LORA">1</a></sup>,
that allow you to fine-tune a small number of parameters of a LLM many parameters without a 
large number of GPUs used for the initial training of these large models. </p>
<p><a href="https://openai.com/">OpenAI</a> allows you to train one of their models through their API
and custom GPTs. Google Gemni offers fine-tuning through the <a href="https://ai.google.dev/gemini-api/docs/model-tuning">Gemini API</a>
as does Anthropic as explained in their <a href="https://www.anthropic.com/news/fine-tune-claude-3-haiku">documentation</a>.</p>
<p>Training <a href="https://ai.meta.com/">LLaMA</a> models locally on a personal computer is possible
depending on the capacities of your local computer. Unfortunately, the process isn't
easy or straight-forward and require running Python code. If you 
have an Apple computer, the <a href="https://github.com/ml-explore/mlx-examples">mlx-lm</a> package,<br />
which uses Apple Silicon GPU, can be used for fine-tuning open-source models like LLaMA. Another
possibility is using HuggingFace's <a href="https://github.com/huggingface/peft">peft</a> package along 
with HuggingFace's transformers to fine-tune models.</p>
<h2>Steps for Fine-tuning</h2>
<p>OpenAI provides directions for [fine-tuning]<sup id="fnref:OPENAI_FINETUNE"><a class="footnote-ref" href="#fn:OPENAI_FINETUNE">2</a></sup> a ChatGPT model that are 
general enough for most fine-tuning tasks:</p>
<ol>
<li>Prepare and upload training data</li>
<li>Train a new fine-tuned model</li>
<li>Evaluate results and go back to step 1 if needed</li>
<li>Use your fine-tuned model</li>
</ol>
<h3>Create Inventory Instance Training Set</h3>
<p>We will create a training set 1,000 random records from the FOLIO community's <a href="https://bugfest-quesnelia.int.aws.folio.org/">Quesnelia Bugfest</a> 
instance. The training set consists of the denormalize Instance along with a text prompt.</p>
<p>Depending on the model and service, you may need reformat the training set to match the expected inputs
to the model. </p>
<p>For OpenAI, the expected format are JSON objects, typically saved one per line in the JSON-L
format, that would like the following for the training set:</p>
<pre><code class="language-javascript">{&quot;messages&quot;: [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;As an expert cataloger, you will create the FOLIO inventory JSON record.&quot;}, 
              {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;For Vacancies in certain judgeships. Published in 1935 by [publisher not identified], Washington Subjects are Courts, Judges, United States--Officials and employees, Electronic books.&quot;},
              {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;{'title': 'Vacancies in certain judgeships.', 'contributors': [], 'subjects': [{'value': 'Courts'}, {'value': 'Judges'}, {'value': 'United States--Officials and employees'}, {'value': 'Electronic books'}], 'classifications': [], 'publication': [{'publisher': '[publisher not identified]', 'place': 'Washington', 'dateOfPublication': '1935', 'role': 'Publication'}], 'instanceTypeText': 'text', 'modeOfIssuanceText': 'single unit'}}&quot;}]}
</code></pre>
<p>For training using <a href="https://github.com/ml-explore/mlx-examples">mlx-lm</a>, the data format is simpler than the OpenAI format but still uses JSON-L:</p>
<pre><code class="language-javascript">{&quot;text&quot;: &quot;As an expert cataloger, you will create the FOLIO inventory JSON record. Q:For Statistical applications for chemistry, manufacturing and controls (CMC) in the pharmaceutical industry /Richard K. Burdick [and others]. Published in 2017 by Springer, Cham A:{'title': 'Statistical applications for chemistry, manufacturing and controls (CMC) in the pharmaceutical industry /Richard K. Burdick [and others].', 'contributors': [], 'publication': [{'publisher': 'Springer', 'place': 'Cham :', 'dateOfPublication': '2017', 'role': None}], 'instanceTypeText': 'text', 'modeOfIssuanceText': 'Monograph'}&quot;}
</code></pre>
<h3>Training</h3>
<p>To train a ChatGPT model after creating a training set, follow these steps:</p>
<ol>
<li>Upload a training file to the <a href="https://platform.openai.com/docs/api-reference/files/create">Files API</a></li>
<li>Create a fine-tune job either through the OpenAI <a href="https://platform.openai.com/finetune">User Interface</a> or through
   Python or Node.js SDK.</li>
</ol>
<p>There are different methods for fine-tuning local LLaMA models. With Apple's Python <a href="https://github.com/ml-explore/mlx-examples">mlx-lm</a>, follow
these steps in <a href="https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/LORA.md">LoRA documentation</a>.</p>
<p>On other platforms and assuming you have a supported GPU, <a href="https://unsloth.ai/">unsloth</a> is a good option. 
You can use <a href="https://unsloth.ai/">unsloth</a> on public cloud providers as well.  Huggingface's 
<a href="https://github.com/huggingface/peft">peft</a> is another good choice and includes the <a href="https://github.com/microsoft/LoRA">loralib</a> package by 
the authors of the original LoRA paper<sup id="fnref:LoRA_PAPER"><a class="footnote-ref" href="#fn:LoRA_PAPER">3</a></sup>. Both need to be used with <a href="https://pytorch.org/">Pytorch</a> to 
fine-tune models as well. </p>
<h3>Using a GPT4ALL Model</h3>
<p>Once you have a fine-tuned model and you have installed <a href="https://gpt4all.io/">GPT4ALL</a>, you can use that model<br />
on a Mac by copying the fine-tuned models to this location:</p>
<p><code>/Users/{user-name}/Library/Application Support/nomic.ai/GPT4All/</code></p>
<h3>Running the Model in Inference Mode</h3>
<div class="footnote">
<hr />
<ol>
<li id="fn:LORA">
<p><a href="https://www.cloudflare.com/learning/ai/what-is-lora/">What is low-rank adaptation (LoRA)?</a>&#160;<a class="footnote-backref" href="#fnref:LORA" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:OPENAI_FINETUNE">
<p><a href="https://platform.openai.com/docs/guides/fine-tuning/">Guide for Fine-tuning</a>&#160;<a class="footnote-backref" href="#fnref:OPENAI_FINETUNE" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:LoRA_PAPER">
<p><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a>&#160;<a class="footnote-backref" href="#fnref:LoRA_PAPER" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:DOCKER">
<p><a href="https://github.com/ggerganov/llama.cpp/blob/master/docs/docker.md">LLaMA.cpp with Docker</a>&#160;<a class="footnote-backref" href="#fnref:DOCKER" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>
  </article>
  <div class="col-3">
    <h4>Navigation</h4>
     <ol><li><a href="/wolfcon-2024-ai-workshop/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/wolfcon-2024-ai-workshop/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/exploring-llms/index.html">Exploring Large Language Models</a><ul><li><a href="chatgpt.html">OpenAI's ChatGPT</a></li><li><a href="claude.html">Anthropic's Claude</a></li><li><a href="gemini.html">Google Gemini</a></li><li><a href="local-llama.html">Meta's Llama</a></li><li><a href="generative-video.html">Generative AI Images and Videos</a></li><li><a href="prompt-engineering.html">Prompt Engineering</a></li><li><a href="retrieval-augmented-generation.html">Retrieval Augmented Generation (RAG)</a></li><li><strong>Training Large Language Models (LLMs)</strong></li></ul></li><li><a href="/wolfcon-2024-ai-workshop/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/wolfcon-2024-ai-workshop/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/wolfcon-2024-ai-workshop/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
  </div>
</div>

    </div>
    <footer style="background-color: #0a3a85; color: white;" >
      <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/sul-dlss-labs/wolfcon-2024-ai-workshop/">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    
  </body>
</html>