<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - Meta's Llama</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <style>

    </style>
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
    
      <div class="row">
        <article class="col-9">
         <h1>Meta's Llama</h1>
<p>In February of 2023, <a href="https://ai.meta.com/">Meta AI</a> (part of the company that includes Facebook) AI released 
an open-source model Large Language Model (LLM) called Llama with a follow-up release in 
April of 2024. Unlike other commercial LLMs, Meta AI released the weights along with a supporting
code to allow for training and other uses not possible with similar models released by OpenAI,
Anthropic, and Google.</p>
<h2>Using Llama</h2>
<p>The easiest way to use Llama is go to https://www.meta.ai and start entering chat prompts. This 
multi-modal web application allows users to generate images and text (although to use the image
generator function you will need to log in with a Facebook account). </p>
<h2>Using Llama Locally</h2>
<p>Because of it's release as open-source, you are able to download the Llama model to run and 
train locally without the service providing additional data to the corporate entity. While there 
are a number of different methods for running Llama locally, a convenient method for running Llama
and other open-source models is using a project called <a href="https://gpt4all.io/">gpt4all</a>. While the computing
requirements can very depending on your laptop's hardware and OS, there are desktop versions
for <a href="https://gpt4all.io/installers/gpt4all-installer-darwin.dmg">Macintosh</a>, 
<a href="https://gpt4all.io/installers/gpt4all-installer-win64.exe">Windows</a>, and 
<a href="https://gpt4all.io/installers/gpt4all-installer-linux.run">Ubuntu</a>. </p>
<h2>Using gpt4all</h2>
<p>For the purposes of this workshop, we will demonstrate a few crucial aspects of using and extending
LLM using <a href="https://gpt4all.io/">gpt4all</a>, including <a href="retrival-augment-generation.md">Retrieval Augmented Generation (RAG)</a> 
and <a href="training-llms.md">training LLMs</a>. If you haven't already prior to this workshop (and if you have
administrative rights on your local laptop), please download and install <a href="https://gpt4all.io/">gpt4all</a>.</p>
<p>A nice feature of <a href="https://gpt4all.io/">gpt4all</a>, is that you can use a locally running LLM without an internet connection. 
This also means that you can restrict access and improve your own security when using RAG on sensitive 
or private documentation.</p>
        </article>
        <div class="col-3">
          <h4>Navigation</h4>
          <ol><li><a href="/wolfcon-2024-ai-workshop/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/wolfcon-2024-ai-workshop/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/exploring-llms/index.html">Exploring Large Language Models</a><ul><li><a href="chatgpt.html">OpenAI's ChatGPT</a></li><li><a href="claude.html">Anthropic's Claude</a></li><li><a href="gemini.html">Google Gemini</a></li><li><strong>Meta's Llama</strong></li><li><a href="generative-video.html">Generative Image and Video</a></li><li><a href="prompt-engineering.html">Prompt Engineering</a></li><li><a href="retrieval-augmented-generation.html">Retrieval Augmented Generation (RAG)</a></li><li><a href="training-llms.html">Training Large Language Models (LLMs)</a></li></ul></li><li><a href="/wolfcon-2024-ai-workshop/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/wolfcon-2024-ai-workshop/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/wolfcon-2024-ai-workshop/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
        </div>
      </div>

    </div>
    <footer style="background-color: #0a3a85; color: white;" >
      <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/jermnelson/wolfcon-02024-ai">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
  </body>
</html>