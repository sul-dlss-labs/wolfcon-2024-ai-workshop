<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - Meta's Llama</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    
   
    <style>

    </style>
   
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
    
<div class="row">
  <article class="col-9">
    <h1>Meta's Llama</h1>
<p>In February 2023, <a href="https://ai.meta.com/">Meta AI</a> (the AI research division of the Meta that also includes Facebook) released 
an open-source Large Language Model (LLM) called Llama with a follow-up release in 
April 2024. Unlike other commercial LLMs, Meta AI released the weights along with supporting
code to allow for training and other uses not possible with similar models released by OpenAI,
Anthropic, and Google.</p>
<h2>Using Llama</h2>
<p>The easiest way to use Llama is go to <a href="https://www.meta.ai">https://www.meta.ai</a> and start entering chat prompts. This 
multi-modal web application allows users to generate images and text (although to use the image
generator function requires logging in with a Facebook account). </p>
<h2>Using Llama Locally</h2>
<p>Because of its release as open-source, you are able to download the Llama model to run and 
train locally without the service providing additional data to the corporate entity. While there 
are a number of ways for running Llama locally, a convenient method for running Llama
and other open-source models is using a project called <a href="https://gpt4all.io/">gpt4all</a>. <a href="https://gpt4all.io/">gpt4all</a>
uses [LLaMA.cpp][LLAMA.CCP] to actually run the models.</p>
<p>While the computing
requirements can vary depending on your laptop's hardware and OS, there are desktop versions
for <a href="https://gpt4all.io/installers/gpt4all-installer-darwin.dmg">Macintosh</a>, 
<a href="https://gpt4all.io/installers/gpt4all-installer-win64.exe">Windows</a>, and 
<a href="https://gpt4all.io/installers/gpt4all-installer-linux.run">Ubuntu</a>.</p>
<h2>Using gpt4all</h2>
<p>For the purposes of this workshop, we will demonstrate a few crucial aspects of using and extending
LLM using <a href="https://gpt4all.io/">gpt4all</a>, including <a href="retrival-augment-generation.md">Retrieval Augmented Generation (RAG)</a> 
and <a href="training-llms.md">training LLMs</a>. If you haven't already prior to this workshop (and if you have
administrative rights on your local laptop), please download and install <a href="https://gpt4all.io/">gpt4all</a>.</p>
<p>A nice feature of <a href="https://gpt4all.io/">gpt4all</a>, is that you can use a locally running LLM without an internet connection. 
This also means that you can restrict access when using RAG on sensitive or private documentation.</p>
<h2>Using Llama.cpp</h2>
<p>The [LLaMA.cpp][LLAMA.CCP] project allows you to run fine-tuned
LLaMA models on your local computer. LLaMA.cpp][LLAMA.CCP] provides an
OpenAI API compatible server that also allows us to integrate with <a href="dspy-docs.vercel.app/">DSPy</a>
and [edge-ai][EDGE_AI] module.</p>
<p>[LLaMA.cpp][LLAMA.CCP] can also be run with [Docker][^DOCKER] on your computer if you 
don't want or can't compile the C++ source code to run on your computer.</p>
<h3>Downloading a LLaMA-based Model</h3>
<p>[LLaMA.cpp][LLAMA.CCP] uses the <a href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">GGUF</a> 
format for model inference and training. Look for GGUF models on [HuggingFace][HUGFACE]. </p>
  </article>
  <div class="col-3">
    <h4>Navigation</h4>
     <ol><li><a href="/wolfcon-2024-ai-workshop/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/wolfcon-2024-ai-workshop/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a></li><li><a href="/wolfcon-2024-ai-workshop/exploring-llms/index.html">Exploring Large Language Models</a><ul><li><a href="chatgpt.html">OpenAI's ChatGPT</a></li><li><a href="claude.html">Anthropic's Claude</a></li><li><a href="gemini.html">Google Gemini</a></li><li><strong>Meta's Llama</strong></li><li><a href="generative-video.html">Generative AI Images and Videos</a></li><li><a href="prompt-engineering.html">Prompt Engineering</a></li><li><a href="retrieval-augmented-generation.html">Retrieval Augmented Generation (RAG)</a></li><li><a href="training-llms.html">Training Large Language Models (LLMs)</a></li></ul></li><li><a href="/wolfcon-2024-ai-workshop/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/wolfcon-2024-ai-workshop/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/wolfcon-2024-ai-workshop/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
  </div>
</div>

    </div>
    <footer style="background-color: #0a3a85; color: white;" >
      <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/sul-dlss-labs/wolfcon-2024-ai-workshop/">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    
  </body>
</html>