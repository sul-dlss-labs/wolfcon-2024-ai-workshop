<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AI Workflows for FOLIO - Transformers</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <style>

    </style>
  </head>
  <body>
    <div class="header">
     <div class="container">
       <h1 style="color: #0a3a85; font-size: 3em;">WOLFcon 2024 - Understanding and Using AI Workflows with FOLIO</h1>
       <h3>23 September 2024</h3>
       <hr>
     </div>
    </div>
    <div class="container">
    
      <div class="row">
        <article class="col-9">
         <h1>Transformers</h1>
<p>Large Language Models (LLMs) are based on a specific type of neural network called a Transformer.
In a simplified transformer model, text is fed into a stack of encoders, where each encoder 
consists of two sub-layers with different weights that process the text as it passes through 
the stack. From there, the text is then sent to a decoder stack, which assembles the output text.</p>
<h2>Transformer Demonstration</h2>
<p>An online explanation of the Transformer architecture is available at 
<a href="https://poloclub.github.io/transformer-explainer/">https://poloclub.github.io/transformer-explainer/</a>, which uses a live GPT-2 model 
to demonstrate how transformers work with an accompanying article<sup id="fnref:EXPLAIN_PAPER"><a class="footnote-ref" href="#fn:EXPLAIN_PAPER">1</a></sup>.</p>
<h2>Encoders</h2>
<p>A transformer with multiple, identical encoders with each is made up of the 
following layers:</p>
<ul>
<li><strong>Self-Attention Layer</strong> - helps the encoder consider the context of each word within the
  input sentence. As the model processes each word self attention allows it to look at
  others positions in the input sequence for clues for improving the encoding of the word.</li>
<li><strong>Feed Forward Neural Network</strong> - outputs from the self-attention layer are fed to 
  and independently applied to this layer.</li>
</ul>
<p>The bottom encoder (#0) is the only encoder that directly creates a vector of the word
through an embedding algorithm. All downstream encoders from the first use the output
of vectors generated from the encoder's feed-forward layer.</p>
<h2>Decoders</h2>
<p>After the text prompt has been fed through the encoders, the outputs from each of the
encoders are feed to the decoders. A decoder is made of three layers:</p>
<ul>
<li><strong>Self-Attention Layer</strong> - same as the encoder, allows the word in context with the other
  words in the input sentence. </li>
<li><strong>Encoder-Decoder Attention</strong> - a separate layer that assists the decoder in focusing on 
  the relevant parts in the input sentence.</li>
<li><strong>Feed Forward Neural Network</strong> </li>
</ul>
<h2>Workshop Use-cases</h2>
<h3>Primary</h3>
<ul>
<li><a href="https://github.com/folio-labs/edge-ai/wiki/Analysis-and-Management-Financial-Orders-and-Invoices">Analysis and Management Financial Orders and Invoices</a></li>
<li><a href="https://github.com/folio-labs/edge-ai/wiki/Automated-Metadata-Generation-Enrichment">Automated Metadata Generation Enrichment</a></li>
<li><a href="https://github.com/folio-labs/edge-ai/wiki/Create-Course-reserves-from-a-csv">Create Course reserves from a csv</a></li>
<li><a href="https://github.com/folio-labs/edge-ai/wiki/Narrative-Circulation-Rules">Narrative Circulation Rules</a></li>
</ul>
<h3>Secondary</h3>
<ul>
<li><a href="https://github.com/folio-labs/ai-workflows/wiki/Collection-Management-and-Optimization">Collection Management and Optimization</a></li>
<li><a href="https://github.com/folio-labs/ai-workflows/wiki/De%E2%80%90duplicate-and-Cluster-Existing-FOLIO-Inventory-Records">De‐duplicate and Cluster Existing FOLIO Inventory Records</a></li>
<li><a href="https://github.com/folio-labs/ai-workflows/wiki/Improving-User-and-Technical-Documentation">Improving User and Technical Documentation</a></li>
</ul>
<h3>Tertiary</h3>
<ul>
<li><a href="https://github.com/folio-labs/ai-workflows/wiki/From-an-Instance-Title,-find-Circ-Status-and-Researve-Item">From an Instance Title, find Circ Status and Researve Item</a></li>
<li><a href="https://github.com/folio-labs/ai-workflows/wiki/Technical-Support-for-Orphaned-Modules">Technical Support for Orphaned Modules</a></li>
<li><a href="https://github.com/folio-labs/ai-workflows/wiki/Information-Discovery">Information Discovery</a></li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:EXPLAIN_PAPER">
<p><a href="https://arxiv.org/abs/2408.04619">Transformer Explainer: Interactive Learning of Text-Generative Models</a>&#160;<a class="footnote-backref" href="#fnref:EXPLAIN_PAPER" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
        </article>
        <div class="col-3">
          <h4>Navigation</h4>
          <ol><li><a href="/intro-pre-conference/index.html">Introduction to WOLFcon AI Pre-conference Workshop</a></li><li><a href="/types-of-ai-ml-relevant-to-libraries/index.html">General Overview of AI and Machine Learning for Libraries</a><ul><li><a href="bayesian.html">Naïve Bayes</a></li><li><a href="natural-language-processing.html">Natural Language Processing</a></li><li><a href="basics-neural-nets.html">Basics of Neural Networks</a></li><li><strong>Transformers</strong></li></ul></li><li><a href="/ethical-considerations-ai-ml-for-libraries/index.html">Ethical Considerations on using AI and Machine Learning for Libraries</a></li><li><a href="/exploring-llms/index.html">Exploring Large Language Models</a></li><li><a href="/folio-ai-use-cases/index.html">FOLIO AI Use Cases</a></li><li><a href="/next-steps-for-adopting-ai-and-ml-in-folio/index.html">Next Steps for Adopting AI and Machine Learning in FOLIO</a></li><li><a href="/recommended-resources-for-further-learning/index.html">Recommended Resources for Further Learning</a></li></ol>
        </div>
      </div>

    </div>
    <footer style="background-color: #0a3a85; color: white;" >
      <div class="container">Original Content by Jeremy Nelson &copy; 2024, <a href="https://github.com/jermnelson/wolfcon-02024-ai">Github</a></div>
    </footer> 
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
  </body>
</html>